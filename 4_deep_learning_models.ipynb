{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Subject model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "import text\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmailInfo=pd.read_csv(\"../email_05.csv\")\n",
    "Subject_content=EmailInfo[[\"subject\",\"spam\"]]\n",
    "Subject_content.fillna(value=\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done.\n",
      "10000 done.\n",
      "20000 done.\n",
      "30000 done.\n",
      "40000 done.\n",
      "50000 done.\n",
      "60000 done.\n",
      "70000 done.\n",
      "80000 done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                fw june 29 -- bna inc. daili labor report\n",
       "1                                          ngx failov plan\n",
       "2                                            intranet site\n",
       "3                           fw ena upstream compani inform\n",
       "4                                        new master physic\n",
       "                               ...                        \n",
       "85198    gb2312 b c2hvzxmgznjvbsb3d3cubg92zwluzmfzaglvb...\n",
       "85199    gb2312 b c2hvzxmgznjvbsb3d3cubg92zwluzmfzaglvb...\n",
       "85200    gb2312 b c2hvzxmgznjvbsb3d3cubg92zwluzmfzaglvb...\n",
       "85201    gb2312 b c2hvzxmgznjvbsb3d3cubg92zwluzmfzaglvb...\n",
       "85202    gb2312 b c2hvzxmgznjvbsb3d3cubg92zwluzmfzaglvb...\n",
       "Name: subject, Length: 85203, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stop_words=list(text.stop_words())\n",
    "def stem_tokenizer(text):\n",
    "    words=[porter_stemmer.stem(token) for token in word_tokenize(text.lower())]\n",
    "    return \" \".join([w for w in words if w not in stop_words])\n",
    "\n",
    "messages=EmailInfo[\"subject\"]\n",
    "for i in range(len(messages)):\n",
    "    messages[i]=stem_tokenizer(str(messages[i]))\n",
    "    if i%10000==0:\n",
    "        print(\"{} done.\".format(i))\n",
    "    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=list(text.stop_words())\n",
    "max_vocab=30000\n",
    "max_len=30\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(messages)\n",
    "sequences = tokenizer.texts_to_sequences(messages)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f814b58e450>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens=[]\n",
    "for seq in sequences:\n",
    "    lens.append(len(seq))\n",
    "lens=pd.DataFrame({\"length\":lens})\n",
    "lens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./subject_feature/vector05.p\",\"wb\") as f:\n",
    "#     pickle.dump(data,f)\n",
    "# with open(\"./subject_feature/label05.p\",\"wb\") as f:\n",
    "#     pickle.dump(pd.get_dummies(EmailInfo[\"spam\"]).values,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmailInfo=pd.read_csv(\"../email_06.csv\")\n",
    "Subject_content=EmailInfo[[\"subject\",\"spam\"]]\n",
    "Subject_content.fillna(value=\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done.\n",
      "10000 done.\n",
      "20000 done.\n",
      "30000 done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        new cathol mail list run\n",
       "1                              12\n",
       "2              take moment explor\n",
       "3                           greet\n",
       "4               loan 3.17 27 term\n",
       "                   ...           \n",
       "35283          watcher daili news\n",
       "35284        number 1 market pick\n",
       "35285              product health\n",
       "35286                            \n",
       "35287              diet plan roof\n",
       "Name: subject, Length: 35288, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stop_words=list(text.stop_words())\n",
    "def stem_tokenizer(text):\n",
    "    words=[porter_stemmer.stem(token) for token in word_tokenize(text.lower())]\n",
    "    return \" \".join([w for w in words if w not in stop_words])\n",
    "\n",
    "messages=EmailInfo[\"subject\"]\n",
    "for i in range(len(messages)):\n",
    "    messages[i]=stem_tokenizer(str(messages[i]))\n",
    "    if i%10000==0:\n",
    "        print(\"{} done.\".format(i))\n",
    "    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(messages)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35283"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens=[]\n",
    "for seq in sequences:\n",
    "    if len(seq)<50:\n",
    "        lens.append(len(seq))\n",
    "lens=pd.DataFrame({\"length\":lens})\n",
    "lens.hist()\n",
    "len(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./subject_feature/vector06.p\",\"wb\") as f:\n",
    "#     pickle.dump(data,f)\n",
    "# with open(\"./subject_feature/label06.p\",\"wb\") as f:\n",
    "#     pickle.dump(pd.get_dummies(EmailInfo[\"spam\"]).values,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data[:10000],pd.get_dummies(EmailInfo[\"spam\"]).values[:10000],shuffle=True,random_state=42,test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This bit of configuration magic is required by some GPUs and not others.\n",
    "# this assumes there is only one gpu, change accordingly if you have more\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if  len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    assert tf.config.experimental.get_memory_growth(physical_devices[0]) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "batchsize=100\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.compat.v1.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def RNN_train(x_train, y_train, x_val, y_val, embedding_mat_columns, test=False):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_vocab,output_dim=embedding_mat_columns,input_length=max_len))\n",
    "    model.add(SimpleRNN(units=embedding_mat_columns,return_sequences=True))\n",
    "    model.add(SimpleRNN(units=embedding_mat_columns))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc', auroc])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "    if not test:\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=batchsize, validation_data=( x_val, y_val ),callbacks=[early_stopping])\n",
    "    else:\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=batchsize, validation_split=0.1, callbacks=[early_stopping])\n",
    "        \n",
    "    y_pred=model.predict(x_val)\n",
    "    f=np.vectorize(lambda x:0 if x<0.5 else 1)\n",
    "    acc=np.mean(f(y_pred)==y_val)\n",
    "    auc=roc_auc_score(y_val,y_pred)\n",
    "    if not test:\n",
    "        print(\"Val accuracy is {0:.3f}. Val auc is {1:.3f}.\".format(acc,auc))\n",
    "    return [0,acc,auc]\n",
    "\n",
    "def LSTM_train(x_train, y_train, x_val, y_val, embedding_mat_columns, test=False):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_vocab,output_dim=embedding_mat_columns,input_length=max_len))\n",
    "    model.add(LSTM(units=embedding_mat_columns,return_sequences=True))\n",
    "    model.add(LSTM(units=embedding_mat_columns))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc', auroc])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "    if not test:\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=batchsize, validation_data=( x_val, y_val ),callbacks=[early_stopping])\n",
    "    else:\n",
    "        model.fit(x_train, y_train, epochs=100, batch_size=batchsize, validation_split=0.1, callbacks=[early_stopping])\n",
    "        \n",
    "    y_pred=model.predict(x_val)\n",
    "    f=np.vectorize(lambda x:0 if x<0.5 else 1)\n",
    "    acc=np.mean(f(y_pred)==y_val)\n",
    "    auc=roc_auc_score(y_val,y_pred)\n",
    "    if not test:\n",
    "        print(\"Val accuracy is {0:.3f}. Val auc is {1:.3f}.\".format(acc,auc))\n",
    "    return [0,acc,auc]\n",
    "\n",
    "def cross_val(model,x_train,y_train,x_test,y_test,cv,embedding_mat_columns):\n",
    "    acclst=[]\n",
    "    auclst=[]\n",
    "#     mask=np.arange(len(x_train))\n",
    "#     random.shuffle(mask)\n",
    "#     x_train,y_train=x_train[mask],y_train[mask]\n",
    "    if cv!=0:\n",
    "        for i in range(cv):\n",
    "            n=len(x_train)\n",
    "            n_val=n//cv\n",
    "            x_val,y_val=x_train[i*n_val:(i+1)*n_val],y_train[i*n_val:(i+1)*n_val]\n",
    "            mask=np.array([True]*len(x_train))\n",
    "            mask[i*n_val:(i+1)*n_val]=False\n",
    "            x_tr,y_tr=x_train[mask],y_train[mask]\n",
    "\n",
    "            print(\"-------------------------------------------------------\")\n",
    "            acc=model(x_tr,y_tr,x_val,y_val,embedding_mat_columns)\n",
    "            acclst.append(acc[1])\n",
    "            auclst.append(acc[2])\n",
    "        \n",
    "    acc=model(x_train,y_train,x_test,y_test,embedding_mat_columns,True)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    if cv!=0:\n",
    "        print(\"5-fold accuracy is {0:.3f}. 5-fold auc is {1:.3f}.\".format(np.mean(acclst),np.mean(auclst)))\n",
    "    print(\"test accuracy is {0:.3f}. test auc is {1:.3f}.\".format(acc[1],acc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 3s 447us/step - loss: 0.4562 - acc: 0.7876 - auroc: 0.7916 - val_loss: 0.2908 - val_acc: 0.8553 - val_auroc: 0.9456\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 3s 421us/step - loss: 0.1141 - acc: 0.9566 - auroc: 0.9942 - val_loss: 0.1861 - val_acc: 0.9141 - val_auroc: 0.9709\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 3s 434us/step - loss: 0.0437 - acc: 0.9844 - auroc: 0.9988 - val_loss: 0.1694 - val_acc: 0.9271 - val_auroc: 0.9776\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 3s 459us/step - loss: 0.0276 - acc: 0.9899 - auroc: 0.9994 - val_loss: 0.1713 - val_acc: 0.9271 - val_auroc: 0.9791\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 3s 451us/step - loss: 0.0223 - acc: 0.9907 - auroc: 0.9996 - val_loss: 0.1792 - val_acc: 0.9253 - val_auroc: 0.9789\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 3s 444us/step - loss: 0.0206 - acc: 0.9903 - auroc: 0.9995 - val_loss: 0.1831 - val_acc: 0.9235 - val_auroc: 0.9779\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 3s 446us/step - loss: 0.0195 - acc: 0.9916 - auroc: 0.9996 - val_loss: 0.2001 - val_acc: 0.9224 - val_auroc: 0.9774\n",
      "Epoch 8/100\n",
      "6800/6800 [==============================] - 3s 435us/step - loss: 0.0191 - acc: 0.9903 - auroc: 0.9996 - val_loss: 0.1958 - val_acc: 0.9253 - val_auroc: 0.9781\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n",
      "Val accuracy is 0.927. Val auc is 0.977.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 3s 485us/step - loss: 0.3954 - acc: 0.8169 - auroc: 0.8546 - val_loss: 0.1558 - val_acc: 0.9318 - val_auroc: 0.9809\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 3s 463us/step - loss: 0.0883 - acc: 0.9668 - auroc: 0.9952 - val_loss: 0.1114 - val_acc: 0.9541 - val_auroc: 0.9868\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 3s 439us/step - loss: 0.0433 - acc: 0.9832 - auroc: 0.9989 - val_loss: 0.1232 - val_acc: 0.9512 - val_auroc: 0.9864\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 3s 416us/step - loss: 0.0311 - acc: 0.9872 - auroc: 0.9991 - val_loss: 0.1213 - val_acc: 0.9500 - val_auroc: 0.9856\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 3s 415us/step - loss: 0.0258 - acc: 0.9885 - auroc: 0.9994 - val_loss: 0.1328 - val_acc: 0.9518 - val_auroc: 0.9857\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 3s 414us/step - loss: 0.0238 - acc: 0.9897 - auroc: 0.9994 - val_loss: 0.1394 - val_acc: 0.9482 - val_auroc: 0.9830\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 3s 415us/step - loss: 0.0224 - acc: 0.9893 - auroc: 0.9994 - val_loss: 0.1423 - val_acc: 0.9512 - val_auroc: 0.9851\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n",
      "Val accuracy is 0.954. Val auc is 0.988.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 3s 460us/step - loss: 0.4648 - acc: 0.7950 - auroc: 0.7603 - val_loss: 0.3193 - val_acc: 0.8582 - val_auroc: 0.9181\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 3s 411us/step - loss: 0.1219 - acc: 0.9513 - auroc: 0.9920 - val_loss: 0.1926 - val_acc: 0.9188 - val_auroc: 0.9670\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 3s 414us/step - loss: 0.0433 - acc: 0.9813 - auroc: 0.9988 - val_loss: 0.2184 - val_acc: 0.9035 - val_auroc: 0.9645\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 3s 416us/step - loss: 0.0283 - acc: 0.9888 - auroc: 0.9996 - val_loss: 0.2120 - val_acc: 0.9124 - val_auroc: 0.9672\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 3s 418us/step - loss: 0.0240 - acc: 0.9904 - auroc: 0.9995 - val_loss: 0.2224 - val_acc: 0.9112 - val_auroc: 0.9660\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 3s 418us/step - loss: 0.0216 - acc: 0.9897 - auroc: 0.9996 - val_loss: 0.2092 - val_acc: 0.9241 - val_auroc: 0.9689\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 3s 418us/step - loss: 0.0206 - acc: 0.9893 - auroc: 0.9995 - val_loss: 0.2201 - val_acc: 0.9224 - val_auroc: 0.9686\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n",
      "Val accuracy is 0.919. Val auc is 0.967.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 3s 469us/step - loss: 0.4329 - acc: 0.8022 - auroc: 0.8285 - val_loss: 0.2240 - val_acc: 0.9271 - val_auroc: 0.9756\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 3s 417us/step - loss: 0.1025 - acc: 0.9603 - auroc: 0.9943 - val_loss: 0.1345 - val_acc: 0.9465 - val_auroc: 0.9867\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 3s 419us/step - loss: 0.0426 - acc: 0.9851 - auroc: 0.9985 - val_loss: 0.1186 - val_acc: 0.9506 - val_auroc: 0.9888\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 3s 422us/step - loss: 0.0289 - acc: 0.9893 - auroc: 0.9992 - val_loss: 0.1206 - val_acc: 0.9512 - val_auroc: 0.9886\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 3s 421us/step - loss: 0.0238 - acc: 0.9910 - auroc: 0.9995 - val_loss: 0.1396 - val_acc: 0.9447 - val_auroc: 0.9878\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 3s 424us/step - loss: 0.0210 - acc: 0.9915 - auroc: 0.9995 - val_loss: 0.1337 - val_acc: 0.9476 - val_auroc: 0.9871\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 3s 424us/step - loss: 0.0210 - acc: 0.9903 - auroc: 0.9995 - val_loss: 0.1501 - val_acc: 0.9441 - val_auroc: 0.9853\n",
      "Epoch 8/100\n",
      "6800/6800 [==============================] - 3s 422us/step - loss: 0.0214 - acc: 0.9899 - auroc: 0.9996 - val_loss: 0.1528 - val_acc: 0.9418 - val_auroc: 0.9835\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n",
      "Val accuracy is 0.951. Val auc is 0.988.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 3s 466us/step - loss: 0.4472 - acc: 0.7943 - auroc: 0.8147 - val_loss: 0.2673 - val_acc: 0.8859 - val_auroc: 0.9498\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 3s 416us/step - loss: 0.1056 - acc: 0.9628 - auroc: 0.9954 - val_loss: 0.1661 - val_acc: 0.9282 - val_auroc: 0.9769\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 3s 415us/step - loss: 0.0436 - acc: 0.9841 - auroc: 0.9988 - val_loss: 0.1699 - val_acc: 0.9306 - val_auroc: 0.9787\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 3s 419us/step - loss: 0.0301 - acc: 0.9878 - auroc: 0.9993 - val_loss: 0.1917 - val_acc: 0.9265 - val_auroc: 0.9754\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 3s 418us/step - loss: 0.0245 - acc: 0.9888 - auroc: 0.9995 - val_loss: 0.2058 - val_acc: 0.9229 - val_auroc: 0.9738\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 3s 418us/step - loss: 0.0219 - acc: 0.9909 - auroc: 0.9996 - val_loss: 0.2081 - val_acc: 0.9282 - val_auroc: 0.9766\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 3s 418us/step - loss: 0.0204 - acc: 0.9912 - auroc: 0.9995 - val_loss: 0.2137 - val_acc: 0.9271 - val_auroc: 0.9771\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n",
      "Val accuracy is 0.928. Val auc is 0.977.\n",
      "Train on 7650 samples, validate on 850 samples\n",
      "Epoch 1/100\n",
      "7650/7650 [==============================] - 3s 456us/step - loss: 0.3763 - acc: 0.8305 - auroc: 0.8667 - val_loss: 0.1891 - val_acc: 0.9188 - val_auroc: 0.9698\n",
      "Epoch 2/100\n",
      "7650/7650 [==============================] - 3s 409us/step - loss: 0.0825 - acc: 0.9686 - auroc: 0.9949 - val_loss: 0.1522 - val_acc: 0.9400 - val_auroc: 0.9794\n",
      "Epoch 3/100\n",
      "7650/7650 [==============================] - 3s 411us/step - loss: 0.0403 - acc: 0.9850 - auroc: 0.9987 - val_loss: 0.1683 - val_acc: 0.9412 - val_auroc: 0.9790\n",
      "Epoch 4/100\n",
      "7650/7650 [==============================] - 3s 411us/step - loss: 0.0286 - acc: 0.9886 - auroc: 0.9993 - val_loss: 0.1899 - val_acc: 0.9365 - val_auroc: 0.9774\n",
      "Epoch 5/100\n",
      "7650/7650 [==============================] - 3s 408us/step - loss: 0.0232 - acc: 0.9894 - auroc: 0.9995 - val_loss: 0.2069 - val_acc: 0.9282 - val_auroc: 0.9763\n",
      "Epoch 6/100\n",
      "7650/7650 [==============================] - 3s 410us/step - loss: 0.0220 - acc: 0.9897 - auroc: 0.9995 - val_loss: 0.2147 - val_acc: 0.9353 - val_auroc: 0.9755\n",
      "Epoch 7/100\n",
      "7650/7650 [==============================] - 3s 411us/step - loss: 0.0234 - acc: 0.9899 - auroc: 0.9996 - val_loss: 0.2223 - val_acc: 0.9376 - val_auroc: 0.9745\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n",
      "-------------------------------------------------------\n",
      "5-fold accuracy is 0.936. 5-fold auc is 0.979.\n",
      "test accuracy is 0.946. test auc is 0.979.\n"
     ]
    }
   ],
   "source": [
    "cross_val(RNN_train,x_train,y_train,x_test,y_test,cv=5,embedding_mat_columns=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM summary\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_vocab,output_dim=embedding_mat_columns,input_length=max_len))\n",
    "model.add(LSTM(units=embedding_mat_columns,return_sequences=True))\n",
    "model.add(LSTM(units=embedding_mat_columns))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc', auroc])\n",
    "model.summary()\n",
    "\n",
    "# RNN summary\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_vocab,output_dim=embedding_mat_columns,input_length=max_len))\n",
    "model.add(LSTM(units=embedding_mat_columns,return_sequences=True))\n",
    "model.add(LSTM(units=embedding_mat_columns))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc', auroc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 6s 940us/step - loss: 0.4855 - acc: 0.7884 - auroc: 0.7573 - val_loss: 0.2664 - val_acc: 0.8994 - val_auroc: 0.9701\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 6s 863us/step - loss: 0.1434 - acc: 0.9504 - auroc: 0.9892 - val_loss: 0.1390 - val_acc: 0.9353 - val_auroc: 0.9829\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 6s 864us/step - loss: 0.0592 - acc: 0.9760 - auroc: 0.9970 - val_loss: 0.1356 - val_acc: 0.9412 - val_auroc: 0.9848\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 6s 864us/step - loss: 0.0369 - acc: 0.9862 - auroc: 0.9987 - val_loss: 0.1502 - val_acc: 0.9394 - val_auroc: 0.9846\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 6s 868us/step - loss: 0.0288 - acc: 0.9891 - auroc: 0.9991 - val_loss: 0.1883 - val_acc: 0.9371 - val_auroc: 0.9829\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 6s 865us/step - loss: 0.0255 - acc: 0.9896 - auroc: 0.9994 - val_loss: 0.1997 - val_acc: 0.9371 - val_auroc: 0.9827\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 6s 866us/step - loss: 0.0226 - acc: 0.9894 - auroc: 0.9994 - val_loss: 0.1967 - val_acc: 0.9365 - val_auroc: 0.9818\n",
      "Epoch 8/100\n",
      "6800/6800 [==============================] - 6s 871us/step - loss: 0.0222 - acc: 0.9903 - auroc: 0.9996 - val_loss: 0.1967 - val_acc: 0.9359 - val_auroc: 0.9818\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n",
      "Val accuracy is 0.941. Val auc is 0.984.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 7s 963us/step - loss: 0.5023 - acc: 0.7766 - auroc: 0.7340 - val_loss: 0.2708 - val_acc: 0.8982 - val_auroc: 0.9818\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 6s 889us/step - loss: 0.1545 - acc: 0.9372 - auroc: 0.9877 - val_loss: 0.1191 - val_acc: 0.9582 - val_auroc: 0.9870\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 6s 894us/step - loss: 0.0642 - acc: 0.9750 - auroc: 0.9970 - val_loss: 0.1158 - val_acc: 0.9553 - val_auroc: 0.9877\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 6s 894us/step - loss: 0.0438 - acc: 0.9837 - auroc: 0.9985 - val_loss: 0.1137 - val_acc: 0.9594 - val_auroc: 0.9872\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 6s 896us/step - loss: 0.0333 - acc: 0.9865 - auroc: 0.9991 - val_loss: 0.1245 - val_acc: 0.9553 - val_auroc: 0.9871\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 6s 901us/step - loss: 0.0277 - acc: 0.9884 - auroc: 0.9994 - val_loss: 0.1492 - val_acc: 0.9512 - val_auroc: 0.9858\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 6s 899us/step - loss: 0.0252 - acc: 0.9894 - auroc: 0.9994 - val_loss: 0.1472 - val_acc: 0.9512 - val_auroc: 0.9851\n",
      "Epoch 8/100\n",
      "6800/6800 [==============================] - 6s 906us/step - loss: 0.0243 - acc: 0.9882 - auroc: 0.9994 - val_loss: 0.1608 - val_acc: 0.9541 - val_auroc: 0.9849\n",
      "Epoch 9/100\n",
      "6800/6800 [==============================] - 6s 904us/step - loss: 0.0236 - acc: 0.9884 - auroc: 0.9994 - val_loss: 0.1755 - val_acc: 0.9500 - val_auroc: 0.9844\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00009: early stopping\n",
      "Val accuracy is 0.959. Val auc is 0.988.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 7s 957us/step - loss: 0.5064 - acc: 0.7704 - auroc: 0.7512 - val_loss: 0.3327 - val_acc: 0.8794 - val_auroc: 0.9715\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 6s 885us/step - loss: 0.1550 - acc: 0.9363 - auroc: 0.9881 - val_loss: 0.1456 - val_acc: 0.9429 - val_auroc: 0.9813\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 6s 882us/step - loss: 0.0705 - acc: 0.9722 - auroc: 0.9964 - val_loss: 0.1377 - val_acc: 0.9512 - val_auroc: 0.9840\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 6s 894us/step - loss: 0.0446 - acc: 0.9837 - auroc: 0.9981 - val_loss: 0.1365 - val_acc: 0.9518 - val_auroc: 0.9862\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 6s 890us/step - loss: 0.0354 - acc: 0.9857 - auroc: 0.9992 - val_loss: 0.1295 - val_acc: 0.9524 - val_auroc: 0.9860\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 6s 908us/step - loss: 0.0286 - acc: 0.9881 - auroc: 0.9994 - val_loss: 0.1549 - val_acc: 0.9424 - val_auroc: 0.9859\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 6s 894us/step - loss: 0.0246 - acc: 0.9884 - auroc: 0.9995 - val_loss: 0.1633 - val_acc: 0.9512 - val_auroc: 0.9855\n",
      "Epoch 8/100\n",
      "6800/6800 [==============================] - 6s 896us/step - loss: 0.0222 - acc: 0.9899 - auroc: 0.9996 - val_loss: 0.1640 - val_acc: 0.9518 - val_auroc: 0.9851\n",
      "Epoch 9/100\n",
      "6800/6800 [==============================] - 6s 896us/step - loss: 0.0216 - acc: 0.9907 - auroc: 0.9996 - val_loss: 0.1815 - val_acc: 0.9482 - val_auroc: 0.9851\n",
      "Epoch 10/100\n",
      "6800/6800 [==============================] - 6s 894us/step - loss: 0.0233 - acc: 0.9887 - auroc: 0.9996 - val_loss: 0.1660 - val_acc: 0.9541 - val_auroc: 0.9857- loss: 0.0216 - acc: 0.988\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00010: early stopping\n",
      "Val accuracy is 0.952. Val auc is 0.986.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 7s 976us/step - loss: 0.5135 - acc: 0.7753 - auroc: 0.7407 - val_loss: 0.3594 - val_acc: 0.8029 - val_auroc: 0.9726\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 6s 895us/step - loss: 0.1763 - acc: 0.9412 - auroc: 0.9888 - val_loss: 0.1361 - val_acc: 0.9465 - val_auroc: 0.9841\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 6s 901us/step - loss: 0.0706 - acc: 0.9737 - auroc: 0.9959 - val_loss: 0.1199 - val_acc: 0.9524 - val_auroc: 0.9867\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 6s 909us/step - loss: 0.0447 - acc: 0.9838 - auroc: 0.9984 - val_loss: 0.1254 - val_acc: 0.9435 - val_auroc: 0.9877\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 6s 903us/step - loss: 0.0340 - acc: 0.9875 - auroc: 0.9989 - val_loss: 0.1305 - val_acc: 0.9559 - val_auroc: 0.9866\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 6s 909us/step - loss: 0.0286 - acc: 0.9881 - auroc: 0.9992 - val_loss: 0.1361 - val_acc: 0.9518 - val_auroc: 0.9862\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 6s 907us/step - loss: 0.0260 - acc: 0.9897 - auroc: 0.9993 - val_loss: 0.1444 - val_acc: 0.9471 - val_auroc: 0.9870\n",
      "Epoch 8/100\n",
      "6800/6800 [==============================] - 6s 909us/step - loss: 0.0237 - acc: 0.9894 - auroc: 0.9996 - val_loss: 0.1533 - val_acc: 0.9465 - val_auroc: 0.9870\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n",
      "Val accuracy is 0.952. Val auc is 0.987.\n",
      "-------------------------------------------------------\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/100\n",
      "6800/6800 [==============================] - 7s 975us/step - loss: 0.4997 - acc: 0.7715 - auroc: 0.7542 - val_loss: 0.3090 - val_acc: 0.8847 - val_auroc: 0.9673\n",
      "Epoch 2/100\n",
      "6800/6800 [==============================] - 6s 901us/step - loss: 0.1481 - acc: 0.9460 - auroc: 0.9894 - val_loss: 0.1589 - val_acc: 0.9400 - val_auroc: 0.9789\n",
      "Epoch 3/100\n",
      "6800/6800 [==============================] - 6s 900us/step - loss: 0.0636 - acc: 0.9757 - auroc: 0.9965 - val_loss: 0.1697 - val_acc: 0.9365 - val_auroc: 0.9817\n",
      "Epoch 4/100\n",
      "6800/6800 [==============================] - 6s 908us/step - loss: 0.0411 - acc: 0.9838 - auroc: 0.9987 - val_loss: 0.1694 - val_acc: 0.9353 - val_auroc: 0.9827\n",
      "Epoch 5/100\n",
      "6800/6800 [==============================] - 6s 910us/step - loss: 0.0313 - acc: 0.9879 - auroc: 0.9991 - val_loss: 0.1829 - val_acc: 0.9259 - val_auroc: 0.9822\n",
      "Epoch 6/100\n",
      "6800/6800 [==============================] - 6s 916us/step - loss: 0.0283 - acc: 0.9866 - auroc: 0.9993 - val_loss: 0.1993 - val_acc: 0.9412 - val_auroc: 0.9823\n",
      "Epoch 7/100\n",
      "6800/6800 [==============================] - 6s 904us/step - loss: 0.0249 - acc: 0.9891 - auroc: 0.9995 - val_loss: 0.2114 - val_acc: 0.9371 - val_auroc: 0.9825\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n",
      "Val accuracy is 0.940. Val auc is 0.979.\n",
      "Train on 7650 samples, validate on 850 samples\n",
      "Epoch 1/100\n",
      "7650/7650 [==============================] - 7s 922us/step - loss: 0.4446 - acc: 0.8069 - auroc: 0.7828 - val_loss: 0.2261 - val_acc: 0.9176 - val_auroc: 0.9656\n",
      "Epoch 2/100\n",
      "7650/7650 [==============================] - 7s 862us/step - loss: 0.1115 - acc: 0.9582 - auroc: 0.9914 - val_loss: 0.1743 - val_acc: 0.9365 - val_auroc: 0.9787\n",
      "Epoch 3/100\n",
      "7650/7650 [==============================] - 7s 876us/step - loss: 0.0536 - acc: 0.9804 - auroc: 0.9976 - val_loss: 0.1987 - val_acc: 0.9329 - val_auroc: 0.9811\n",
      "Epoch 4/100\n",
      "7650/7650 [==============================] - 7s 926us/step - loss: 0.0398 - acc: 0.9843 - auroc: 0.9986 - val_loss: 0.1658 - val_acc: 0.9376 - val_auroc: 0.9816 0.0310 - acc: 0.9892 - auro - ETA: 3s \n",
      "Epoch 5/100\n",
      "7650/7650 [==============================] - 7s 932us/step - loss: 0.0304 - acc: 0.9880 - auroc: 0.9992 - val_loss: 0.1973 - val_acc: 0.9353 - val_auroc: 0.9827\n",
      "Epoch 6/100\n",
      "7650/7650 [==============================] - 7s 936us/step - loss: 0.0261 - acc: 0.9884 - auroc: 0.9995 - val_loss: 0.1936 - val_acc: 0.9412 - val_auroc: 0.9812\n",
      "Epoch 7/100\n",
      "7650/7650 [==============================] - 7s 934us/step - loss: 0.0238 - acc: 0.9893 - auroc: 0.9995 - val_loss: 0.2184 - val_acc: 0.9318 - val_auroc: 0.9810\n",
      "Epoch 8/100\n",
      "7650/7650 [==============================] - 7s 941us/step - loss: 0.0223 - acc: 0.9902 - auroc: 0.9995 - val_loss: 0.2318 - val_acc: 0.9376 - val_auroc: 0.9783\n",
      "Epoch 9/100\n",
      "7650/7650 [==============================] - 7s 941us/step - loss: 0.0211 - acc: 0.9903 - auroc: 0.9995 - val_loss: 0.2515 - val_acc: 0.9271 - val_auroc: 0.9773\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00009: early stopping\n",
      "-------------------------------------------------------\n",
      "5-fold accuracy is 0.949. 5-fold auc is 0.985.\n",
      "test accuracy is 0.953. test auc is 0.981.\n"
     ]
    }
   ],
   "source": [
    "cross_val(LSTM_train,x_train,y_train,x_test,y_test,cv=5,embedding_mat_columns=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM subject test all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This bit of configuration magic is required by some GPUs and not others.\n",
    "# this assumes there is only one gpu, change accordingly if you have more\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if  len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    assert tf.config.experimental.get_memory_growth(physical_devices[0]) == True\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((85203, 30), (85203, 2), (35288, 30), (35288, 2))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab=30000\n",
    "max_len=30\n",
    "\n",
    "with open(\"./subject_feature/vector05.p\",\"rb\") as f:\n",
    "    x_train=pickle.load(f)\n",
    "with open(\"./subject_feature/label05.p\",\"rb\") as f:\n",
    "    y_train=pickle.load(f)\n",
    "    \n",
    "with open(\"./subject_feature/vector06.p\",\"rb\") as f:\n",
    "    x_test=pickle.load(f)\n",
    "with open(\"./subject_feature/label06.p\",\"rb\") as f:\n",
    "    y_test=pickle.load(f)\n",
    "    \n",
    "x_train=x_train\n",
    "y_train=y_train\n",
    "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from notebook_utils import get_logger,LoggingCallback\n",
    "\n",
    "# helper routing to log a message with time\n",
    "def log_message( label_string ):\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp( ts ).strftime( '%Y-%m-%d %H:%M:%S:%f' )\n",
    "    print(\"{}: {}\".format(st, label_string))\n",
    "    with open(\"training_log.txt\",\"a\") as f:\n",
    "        f.write(\"{}: {}\\n\".format(st, label_string))\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.compat.v1.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def LSTM_train(x_train, y_train, x_val, y_val, embedding_mat_columns,name):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_vocab,output_dim=embedding_mat_columns,input_length=max_len))\n",
    "    model.add(LSTM(units=embedding_mat_columns,return_sequences=True))\n",
    "    model.add(LSTM(units=embedding_mat_columns))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    opt=Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['acc', ])#auroc\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "    result=model.fit(x_train, y_train, epochs=100, batch_size=batchsize, validation_split=0.1, #validation_data=(x_val, y_val), \n",
    "                     callbacks=[early_stopping,LoggingCallback(3,logger)])\n",
    "    \n",
    "    model.save(\"./models/LSTM_model_{}.format(name)\")\n",
    "    \n",
    "    with open(\"./models/LSTM_result_{}.p\".format(name),\"wb\") as f:\n",
    "        pickle.dump(result.history,f)\n",
    "\n",
    "def train_all(model,x_train,y_train,x_test,y_test,embedding_mat_columns,name):\n",
    "    log_message( \"start\" )\n",
    "    model(x_train,y_train,x_test,y_test,embedding_mat_columns,name)\n",
    "    log_message( \"end\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-26 17:23:16:699507: start\n",
      "Train on 76682 samples, validate on 8521 samples\n",
      "Epoch 1/100\n",
      "76682/76682 [==============================] - 40s 523us/step - loss: 0.2159 - acc: 0.9018 - val_loss: 0.2135 - val_acc: 0.9033\n",
      "2020-04-26 17:23:57|\t0: TRAIN loss 0.21594813  ||  VAL loss 0.21354558\n",
      "Epoch 2/100\n",
      "76682/76682 [==============================] - 39s 513us/step - loss: 0.1109 - acc: 0.9525 - val_loss: 0.2186 - val_acc: 0.9002\n",
      "Epoch 3/100\n",
      "76682/76682 [==============================] - 38s 496us/step - loss: 0.0867 - acc: 0.9612 - val_loss: 0.2217 - val_acc: 0.9079\n",
      "Epoch 4/100\n",
      "76682/76682 [==============================] - 38s 492us/step - loss: 0.0758 - acc: 0.9637 - val_loss: 0.2188 - val_acc: 0.9171\n",
      "2020-04-26 17:25:53|\t3: TRAIN loss 0.07579289  ||  VAL loss 0.21879032\n",
      "Epoch 5/100\n",
      "76682/76682 [==============================] - 38s 490us/step - loss: 0.0688 - acc: 0.9674 - val_loss: 0.2164 - val_acc: 0.9149\n",
      "Epoch 6/100\n",
      "76682/76682 [==============================] - 38s 489us/step - loss: 0.0632 - acc: 0.9689 - val_loss: 0.2265 - val_acc: 0.9202\n",
      "Epoch 7/100\n",
      "76682/76682 [==============================] - 38s 492us/step - loss: 0.0594 - acc: 0.9700 - val_loss: 0.3573 - val_acc: 0.8943\n",
      "2020-04-26 17:27:45|\t6: TRAIN loss 0.05941065  ||  VAL loss 0.35733867\n",
      "Epoch 8/100\n",
      "76682/76682 [==============================] - 38s 499us/step - loss: 0.0557 - acc: 0.9710 - val_loss: 0.3213 - val_acc: 0.9095\n",
      "Epoch 9/100\n",
      "76682/76682 [==============================] - 38s 497us/step - loss: 0.0537 - acc: 0.9719 - val_loss: 0.3761 - val_acc: 0.8960\n",
      "Epoch 10/100\n",
      "76682/76682 [==============================] - 38s 496us/step - loss: 0.0519 - acc: 0.9722 - val_loss: 0.4133 - val_acc: 0.9011\n",
      "2020-04-26 17:29:40|\t9: TRAIN loss 0.05187828  ||  VAL loss 0.41334428\n",
      "Epoch 11/100\n",
      "76682/76682 [==============================] - 38s 493us/step - loss: 0.0498 - acc: 0.9735 - val_loss: 0.4092 - val_acc: 0.8954\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00011: early stopping\n",
      "2020-04-26 17:30:18|\t10: TRAIN loss 0.04983855  ||  VAL loss 0.40919105\n",
      "2020-04-26 17:30:18:228026: end\n"
     ]
    }
   ],
   "source": [
    "batchsize=108\n",
    "name=\"subject_body_1\"\n",
    "logger=get_logger(\"LSTM_{}.log\".format(name))\n",
    "train_all(LSTM_train,x_train,y_train,x_test,y_test,embedding_mat_columns=64,name=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
